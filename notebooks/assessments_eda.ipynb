{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration for Reported Assessment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import plotly.graph_objects as go\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import assessment data minus fully suppressed scores\n",
    "assessments = pd.read_pickle('../data/school_based/assessments_clean.pkl')\n",
    "\n",
    "# Import Suppressed outlier data\n",
    "suppressed = pd.read_pickle ('../data/school_based/full_suppression.pkl')\n",
    "\n",
    "# Import Tennessee School District Geometry\n",
    "tn_leas = gpd.read_file('../data/tn_leas.geojson', index_col='system_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listy McListface - A Place to look at the lists in my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assessments Info\n",
    "assessments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Student Groups\n",
    "student_group_list = np.unique(assessments['student_group'].values).tolist()\n",
    "student_group_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# School Types\n",
    "school_type_list = np.unique(assessments['school_type'].values).tolist()\n",
    "school_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject Areas\n",
    "subject_area_list = np.unique(assessments['subject_area'].values).tolist()\n",
    "subject_area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsuppressed: Broad Overview of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Unweighted Proficiencies by School-Level, Subject Area, and Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject Area Pivot Table\n",
    "subject_area_pivot = pd.pivot_table(assessments,\n",
    "                       values = 'pct_met_exceeded',\n",
    "                       index = ['school_lvl'],\n",
    "                       columns = ['subject_area','year'],\n",
    "                       aggfunc = np.mean)\n",
    "\n",
    "# Get the current list of years\n",
    "years = list(subject_area_pivot.columns)\n",
    "\n",
    "subject_area_pivot\n",
    "#.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual of Log Proficiencies For All Subjects Combined by School Level (Unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic scaling to the values\n",
    "log_values = np.log(subject_area_pivot.values)\n",
    "\n",
    "# Create a diverging colorscale for heatmap\n",
    "colorscale = 'Viridis'\n",
    "\n",
    "# Create the heatmap figure\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=log_values,\n",
    "    x=subject_area_pivot.columns.get_level_values('year'),\n",
    "    y=subject_area_pivot.index,\n",
    "    colorscale=colorscale,\n",
    "    zmid=np.median(log_values)  # Set the midpoint of the colorscale\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Subject Area Heatmap (Log Scale)',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='School Level'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Spatial Join of School Geometry (lat/long point) and District (polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some funk going on here.  District names don't match across datasets.  I'm going to do a spatial merge to see which dististricts are associated based on thier physical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CRS for the assessments dataframe\n",
    "assessments.crs = \"EPSG:4269\"\n",
    "\n",
    "# Reproject assessments dataframe to match the CRS of tn_leas dataframe\n",
    "reproject = assessments.to_crs(tn_leas.crs)\n",
    "\n",
    "# Perform spatial join\n",
    "assessments = gpd.sjoin(reproject, tn_leas, how='inner', predicate='intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed a mismatch in naming conventions between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at differences in naming conventions\n",
    "pd.merge(\n",
    "    left = assessments.groupby(['system_name_left','school_name'])['system_name_right'].nunique().loc[lambda x: x>1].reset_index().drop(columns = 'system_name_right'),\n",
    "    right = assessments)[['system_name_left', 'school_name', 'system_name_right']].drop_duplicates()#.to_csv('../data/fixerupper.csv', index = False)\n",
    "\n",
    "subset_assessments = assessments[['system_name_left', 'system_name_right']]\n",
    "subset_assessments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dictionary mapping\n",
    "clean_dictionary =pd.read_csv('../data/clean_dictionary.csv')\n",
    "clean_dictionary = pd.concat([pd.merge(\n",
    "    left = assessments.groupby(['system_name_left','school_name'])['system_name_right'].nunique().loc[lambda x: x == 1].reset_index().drop(columns = 'system_name_right'),\n",
    "    right = assessments)[['system_name_left', 'school_name', 'system_name_right']].drop_duplicates(), clean_dictionary])\n",
    "clean_dictionary.tail(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proccessing steps: Keep system_name_right, rename as system_name, drop system_name_right, set_system_name at col index 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge assessments with the clean dictionary\n",
    "assessments = (pd.merge(left = assessments, right = clean_dictionary))\n",
    "\n",
    "# Rename the 'system_name_right' column to 'system_name'\n",
    "assessments.rename(columns={'system_name_right': 'system_name'}, inplace=True)\n",
    "\n",
    "# Move the 'system_name' column to the third position\n",
    "columns = list(assessments.columns)\n",
    "columns.insert(2, columns.pop(columns.index('system_name')))\n",
    "assessments = assessments[columns]\n",
    "\n",
    "# Drop the 'system_name_right' column\n",
    "assessments.drop('system_name_left', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üè´ List of columns in assessments (school level) for use in district analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's figure how how I can pivot this    \n",
    "districts = assessments[['locale',\n",
    "                         'year',\n",
    "                         'system_name', \n",
    "                         'school_lvl', \n",
    "                         'subject_area', \n",
    "                         'student_group', \n",
    "                         'pct_met_exceeded_w', \n",
    "                         'school_type',\n",
    "                         'magnet',\n",
    "                         'charter',\n",
    "                         'title_1',\n",
    "                         'fte_teachers_w',\n",
    "                         'stu_tchr_ratio_w',\n",
    "                         'valid_tests']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_system_names = districts['system_name'].unique()\n",
    "unique_system_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Weighting Metrics Based on Valid Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìá Indices for Weight Pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for pivots\n",
    "indices = ['system_name',  # School District \n",
    "           'school_lvl', # Level of school (Elem, Middle, High)\n",
    "           'school_type', # Regular, alternative, special education\n",
    "           'magnet', # Is magnet?\n",
    "           'charter', # Is charter?\n",
    "           'title_1', # Is title 1?\n",
    "           'locale', # Location category of school (rural, large city, etc)\n",
    "           'subject_area', # Overall content area of \n",
    "           'student_group'] # Aggregate student groups (all students, students with disabilities, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãÔ∏è‚ûï Sum of Valid-Test-Weighted Scores for pct_met_exceeded, fte_teachers, and student_tchr_ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of valid test scores (The üèãÔ∏è)\n",
    "weight = pd.pivot_table(\n",
    "    districts,\n",
    "    values='valid_tests',  # Column to calculate the sum of valid test scores\n",
    "    index=indices,\n",
    "    columns='year',\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "# Sum of weighted scores pivot for 'pct_met_exceeded'\n",
    "sum_weighted_proficiency = pd.pivot_table(\n",
    "    districts,\n",
    "    values='pct_met_exceeded_w', # Weighted sum of students who met or exceeded expectations\n",
    "    index=indices,\n",
    "    columns='year',\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "# Sum of weighted scores pivot for 'fte_teachers'\n",
    "sum_weighted_fte = pd.pivot_table(\n",
    "    districts,\n",
    "    values='fte_teachers_w', # Weighted sum of full-time equivalent teachers\n",
    "    index=indices,\n",
    "    columns='year',\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "# Sum of weighted scores pivot for 'stu_tchr_ratio'\n",
    "sum_weighted_str = pd.pivot_table(\n",
    "    districts,\n",
    "    values='stu_tchr_ratio_w', # Weighted sum of student/teacher ratios\n",
    "    index=indices,\n",
    "    columns='year',\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "\n",
    "# Create a multi-level column index\n",
    "column_index = pd.MultiIndex.from_product([['pct_met_exceeded', 'fte_teachers', 'stu_tchr_ratio'], sum_weighted_proficiency.columns])\n",
    "\n",
    "# Concatenate the pivot tables horizontally\n",
    "weighted_sums_pivot = pd.concat([sum_weighted_proficiency, sum_weighted_fte, sum_weighted_str], axis=1)\n",
    "weighted_sums_pivot.columns = column_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãÔ∏è‚öñÔ∏è Weighted Averages for pct_met_exceeded_w, fte_teachers_w, stu_tchr_ratio_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide sum_weighted_proficiency by weight\n",
    "weighted_avg_proficiency = sum_weighted_proficiency / weight\n",
    "\n",
    "# Divide sum_weighted_fte by weight\n",
    "weighted_avg_fte = sum_weighted_fte / weight\n",
    "\n",
    "# Divide sum_weighted_str by weight\n",
    "weighted_avg_str = sum_weighted_str / weight\n",
    "\n",
    "# Create a multi-level column index for the weighted average pivots\n",
    "column_index = pd.MultiIndex.from_product([['pct_met_exceeded', 'fte_teachers', 'stu_tchr_ratio'], weighted_avg_proficiency.columns])\n",
    "\n",
    "# Concatenate the weighted average pivots horizontally\n",
    "weighted_avg_pivot = pd.concat([weighted_avg_proficiency, weighted_avg_fte, weighted_avg_str], axis=1)\n",
    "weighted_avg_pivot.columns = column_index\n",
    "\n",
    "weighted_avg_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§∫ Slicing and Lagging\n",
    "> \"... the heavy-sword splendid.  The hard-edg√®d weapon;  with Hrunting to aid me, I shall gain me glory .. \"\n",
    "    >> *-Beowulf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü™ü Setting Variables for lag window slicing\n",
    "> üóíÔ∏è **lvpp lags** ‚Üí The lags for **Science** related metrics require some special treatment. The last valid prepandemic measurement for **ELA**, **Math**, and **Social Studies** was in **2019**.   **Science**, however, was not assessed in **2019**, but was in **2018**.  I cannnot directly compare lag windows for all subjects that start in **2019**.  In order to include metrics related to **Science** for fair comparisons, its window must start at **2018**.  Therefore, **lvpp** variables will be used as treatment for window the **last valid prepandemic measure** for each content area (e.g. 2018 or 2019 respectively).\n",
    "\n",
    "> üóÇÔ∏è lvpp ‚Üí last valid pre-pandemic assessment to 2021 (first full school year after 2020 school closure)\n",
    "\n",
    "> üóÇÔ∏è intra ‚Üí difference between first and second years school were reopend post-pandemic (2021 - 2022)\n",
    "\n",
    "> üóÇÔ∏è pre_post ‚Üí difference between last valid pre-pandemic scores and last year in the dataset (lvpp to 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stop year for all metrics\n",
    "lvpp_stop = 2021\n",
    "\n",
    "# Set the start year for Math, ELA, and Social Studies related metrics\n",
    "lvpp_start_mess = 2019\n",
    "\n",
    "# Set the start year for Science-related metrics\n",
    "lvpp_start_science = 2018\n",
    "\n",
    "# Set the start and stop year for the \"intra\" lag\n",
    "intra_start = 2021\n",
    "intra_stop = 2022\n",
    "\n",
    "# Science Slicer\n",
    "science_slice = (slice(None), slice(None), slice(None), slice(None), slice(None), slice(None), slice(None), 'Science')\n",
    "\n",
    "# Non-Science slicer\n",
    "subjects_slice = (slice(None), slice(None), slice(None), slice(None), slice(None), slice(None), slice(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Assessment Proficiency Lags\n",
    "\n",
    "> üóÇÔ∏è pct_met_exceeded ‚Üí Changes in weighted average full-time students who displayed **at-least minimum expected proficiency** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate separate lag scores for Science-related metrics.\n",
    "\n",
    "# Last Valid Pre-Pandemic met_exceded measurement (lvpp)\n",
    "weighted_avg_pivot.loc[science_slice, ('pct_met_exceeded', 'lvpp')] = (\n",
    "    weighted_avg_pivot.loc[science_slice, ('pct_met_exceeded', lvpp_stop)] -\n",
    "    weighted_avg_pivot.loc[science_slice, ('pct_met_exceeded', lvpp_start_science)]\n",
    ")\n",
    "\n",
    "# Calculate the lvpp scores for Math, ELA, and Social Studies related metrics\n",
    "weighted_avg_pivot.loc[\n",
    "    subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "    ('pct_met_exceeded', 'lvpp')\n",
    "] = (\n",
    "    weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('pct_met_exceeded', lvpp_stop)\n",
    "    ] - weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('pct_met_exceeded', lvpp_start_mess)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the intra lag score for 'pct_met_exceeded'\n",
    "weighted_avg_pivot.loc[subjects_slice, ('pct_met_exceeded', 'intra')] = (\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('pct_met_exceeded', intra_stop)] -\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('pct_met_exceeded', intra_start)]\n",
    ")\n",
    "\n",
    "# Calculate the 'pre-post' assessment lag scores\n",
    "weighted_avg_pivot[('pct_met_exceeded', 'pre-post')] = (\n",
    "    weighted_avg_pivot[('pct_met_exceeded', 2022)] -\n",
    "    weighted_avg_pivot[('pct_met_exceeded', 'lvpp')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Full Time Equivalent Teachers Lag\n",
    "\n",
    "> üóÇÔ∏è fte_teachers ‚Üí Changes in weighted-average of **full-time-equivalent teachers** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Valid Pre-Pandemic science-related full-time equivalent teachers measurement (lvpp)\n",
    "weighted_avg_pivot.loc[science_slice, ('fte_teachers', 'lvpp')] = (\n",
    "    weighted_avg_pivot.loc[science_slice, ('fte_teachers', lvpp_stop)] -\n",
    "    weighted_avg_pivot.loc[science_slice, ('fte_teachers', lvpp_start_science)]\n",
    ")\n",
    "\n",
    "# Last Valid Pre-Pandemic full-time equivalent teacher measurement (lvpp)\n",
    "weighted_avg_pivot.loc[\n",
    "    subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "    ('fte_teachers', 'lvpp')\n",
    "] = (\n",
    "    weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('fte_teachers', lvpp_stop)\n",
    "    ] - weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('fte_teachers', lvpp_start_mess)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the intra lag score for 'fte_teachers'\n",
    "weighted_avg_pivot.loc[subjects_slice, ('fte_teachers', 'intra')] = (\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('fte_teachers', intra_stop)] -\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('fte_teachers', intra_start)]\n",
    ")\n",
    "\n",
    "# Calculate the 'pre-post' assessment lag scores for full-time equivalent teachers\n",
    "weighted_avg_pivot[('fte_teachers', 'pre-post')] = (\n",
    "    weighted_avg_pivot[('fte_teachers', 2022)] -\n",
    "    weighted_avg_pivot[('fte_teachers', 'lvpp')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüéì/üßë‚Äçüè´ Student Teacher Ratio Lag\n",
    "\n",
    "> üóÇÔ∏è stu_tchr_ratio ‚Üí Changes in weighted-average **student-to-teacher ratio** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Valid Pre-Pandemic science-related student/teacher ratio measurement (lvpp)\n",
    "weighted_avg_pivot.loc[science_slice, ('stu_tchr_ratio', 'lvpp')] = (\n",
    "    weighted_avg_pivot.loc[science_slice, ('stu_tchr_ratio', lvpp_stop)] -\n",
    "    weighted_avg_pivot.loc[science_slice, ('stu_tchr_ratio', lvpp_start_science)]\n",
    ")\n",
    "\n",
    "# Last Valid Pre-Pandemic non science-related student/teacher ratio measurement (lvpp)\n",
    "weighted_avg_pivot.loc[\n",
    "    subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "    ('stu_tchr_ratio', 'lvpp')\n",
    "] = (\n",
    "    weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('stu_tchr_ratio', lvpp_stop)\n",
    "    ] - weighted_avg_pivot.loc[\n",
    "        subjects_slice + (['Math', 'ELA', 'Social Studies'],),\n",
    "        ('stu_tchr_ratio', lvpp_start_mess)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate the intra lag score for 'stu_tchr_ratio'\n",
    "weighted_avg_pivot.loc[subjects_slice, ('stu_tchr_ratio', 'intra')] = (\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('stu_tchr_ratio', intra_stop)] -\n",
    "    weighted_avg_pivot.loc[subjects_slice, ('stu_tchr_ratio', intra_start)]\n",
    ")\n",
    "\n",
    "# Calculate the 'pre-post' assessment lag scores for student/teacher ratios\n",
    "weighted_avg_pivot[('stu_tchr_ratio', 'pre-post')] = (\n",
    "    weighted_avg_pivot[('stu_tchr_ratio', 2022)] -\n",
    "    weighted_avg_pivot[('stu_tchr_ratio', 'lvpp')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® \"Un-Pivoting\"  Weighted Average Pivot to Prepare for Geometry \n",
    "üóÇÔ∏è weighted_avg_pivot ‚Üí weighted_average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index of weighted average pivot to create weighted_average_metrics DataFrame\n",
    "weighted_average_metrics = weighted_avg_pivot.reset_index()\n",
    "\n",
    "# Compressing hierarchy in columns and joining levels with \"_\"\n",
    "weighted_average_metrics.columns = ['_'.join(str(col) for col in column) for column in weighted_average_metrics.columns.values]\n",
    "\n",
    "# Removing trailing \"_\" introduced when compressing\n",
    "for col in weighted_average_metrics.columns:\n",
    "        # Check if the column name ends with '_'\n",
    "        if col.endswith('_'):\n",
    "            # If it does, remove the trailing underscore\n",
    "            weighted_average_metrics = weighted_average_metrics.rename(columns={col: col[:-1]})\n",
    "\n",
    "weighted_average_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique system names in weighted assessments\n",
    "weighted_average_metrics_system_names = weighted_average_metrics['system_name'].tolist()\n",
    "\n",
    "# List of unique system names in tn_leas\n",
    "tn_leas_system_names = tn_leas['system_name'].tolist()\n",
    "\n",
    "# Checking if system names exist in weighted assessments and do not in the district boundaries data set\n",
    "unique_in_weighted_average_metrics = list(set(weighted_average_metrics_system_names) - set(tn_leas_system_names))\n",
    "\n",
    "# Check if system name exists in district boundaries, but does not in weighted assessments\n",
    "unique_in_tn_leas = list(set(tn_leas_system_names) - set(weighted_average_metrics_system_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if system names exist in weighted assessments that do not in the district boundaries data set.  I've never been so happy to see an empty list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_in_weighted_average_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  üìä ü§ù üó∫Ô∏è Merging Weighted Average Metrics with District Geometry Files and converting to GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_metrics = pd.merge(left=weighted_average_metrics, right=tn_leas, on=\"system_name\", how='left')\n",
    "weighted_average_metrics = gpd.GeoDataFrame(weighted_average_metrics, geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming columns to conform with ERSI standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rename_dict = {\n",
    "    'pct_met_exceeded_2018': 'pctm_18',\n",
    "    'pct_met_exceeded_2019': 'pctm_19',\n",
    "    'pct_met_exceeded_2021': 'pctm_21',\n",
    "    'pct_met_exceeded_2022': 'pctm_22',\n",
    "    'fte_teachers_2018': 'fte_18',\n",
    "    'fte_teachers_2019': 'fte_19',\n",
    "    'fte_teachers_2021': 'fte_21',\n",
    "    'fte_teachers_2022': 'fte_22',\n",
    "    'stu_tchr_ratio_2018': 'str_18',\n",
    "    'stu_tchr_ratio_2019': 'str_19',\n",
    "    'stu_tchr_ratio_2021': 'str_21',\n",
    "    'stu_tchr_ratio_2022': 'str_22',\n",
    "    'pct_met_exceeded_lvpp': 'pctm_lvpp',\n",
    "    'pct_met_exceeded_intra': 'pctm_intra',\n",
    "    'pct_met_exceeded_pre-post': 'pctm_pp',\n",
    "    'fte_teachers_lvpp': 'fte_lvpp',\n",
    "    'fte_teachers_intra': 'fte_intra',\n",
    "    'fte_teachers_pre-post': 'fte_pp',\n",
    "    'stu_tchr_ratio_lvpp': 'str_lvpp',\n",
    "    'stu_tchr_ratio_intra': 'str_intra',\n",
    "    'stu_tchr_ratio_pre-post': 'str_pp'\n",
    "}\n",
    "weighted_average_metrics.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_metrics.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some funk going on here.  District names don't match across datasets.  I'm going to do a spatial merge to see which dististricts are associated based on thier physical location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü•í Exporting Weighted Average Metrics without Geospatial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAUTION!: Don't turn on the pkl generator unless hungry.\n",
    "weighted_average_metrics.to_pickle('../data/Weighted_average_metrics.pkl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  üó∫Ô∏è Exporting Weighed Average Metrics GeoDataFrame as Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION!: Don't turn on the GeoJSON generator unless lost.\n",
    "# weighted_average_metrics.to_file('../data/weighted_average_metrics.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_average_metrics.to_file('../data/weighted_average_metrics.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visual EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the school level you want to focus on\n",
    "selected_school_level = 'High'\n",
    "\n",
    "# Filter the data for \"All Students\" student group and the selected school level\n",
    "data = weighted_average_metrics[(weighted_average_metrics['student_group'] == 'All Students') & (weighted_average_metrics['school_lvl'] == selected_school_level)]\n",
    "\n",
    "# Get the unique content areas for the selected school level\n",
    "content_areas = data['subject_area'].unique()\n",
    "\n",
    "# Create a 2D array to store the heatmap values\n",
    "z_data = []\n",
    "\n",
    "# Iterate over content areas\n",
    "for content_area in content_areas:\n",
    "    # Filter the data for the current content area\n",
    "    content_area_data = data[data['subject_area'] == content_area]\n",
    "    \n",
    "    # Get the lagged calculation values for the current content area\n",
    "    lagged_values = content_area_data['pctm_pp'].values\n",
    "    \n",
    "    # Append the lagged calculation values to the z_data list, ignoring NaN values\n",
    "    z_data.append([value for value in lagged_values if not np.isnan(value)])\n",
    "\n",
    "# Create the heatmap trace\n",
    "heatmap = go.Heatmap(\n",
    "    x=content_areas,\n",
    "    y=['Lagged Calculation'],\n",
    "    z=z_data,\n",
    "    colorscale='viridis',\n",
    "    colorbar=dict(title='Percentage')\n",
    ")\n",
    "\n",
    "# Create the figure and add the heatmap\n",
    "fig = go.Figure(data=heatmap)\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title=f'Lagged Calculation by Content Area for {selected_school_level} School Level',\n",
    "    xaxis=dict(title='Content Area'),\n",
    "    yaxis=dict(title=''),\n",
    "    height=400,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Show the heatmap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_area = [weighted_average_metrics['subject_area'].unique()]\n",
    "subject_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_lags = weighted_average_metrics[['school_lvl', 'subject_area', 'pctm_lvpp', 'pctm_intra', 'pctm_pp']]\n",
    "average_proficiency = pct_lags.groupby(['school_lvl', 'subject_area']).mean().reset_index()\n",
    "average_proficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average proficiency per school level and content area\n",
    "average_proficiency = weighted_average_metrics.groupby(['school_lvl', 'subject_area']).mean()[['pctm_lvpp', 'pctm_intra', 'pctm_pp']].reset_index()\n",
    "\n",
    "# Define subject_area\n",
    "subject_area = weighted_average_metrics['subject_area'].unique().tolist()\n",
    "\n",
    "# Reorder school levels\n",
    "school_lvl_order = ['Elementary', 'Middle', 'High', 'Secondary', 'Other']\n",
    "\n",
    "# Create bar plot\n",
    "fig = px.bar(average_proficiency, x='subject_area', y=['pctm_lvpp', 'pctm_intra', 'pctm_pp'], color_discrete_map={'pctm_lvpp': 'blue', 'pctm_intra': 'green', 'pctm_pp': 'red'},\n",
    "             facet_row='subject_area', facet_col='school_lvl',\n",
    "             category_orders={'subject_area': subject_area, 'school_lvl': school_lvl_order},\n",
    "             labels={'pctm_lvpp': 'Average Proficiency (LVPP)',\n",
    "                     'pctm_intra': 'Average Proficiency (Intra)',\n",
    "                     'pctm_pp': 'Average Proficiency (PP)',\n",
    "                     'school_lvl': 'School Level',\n",
    "                     'subject_area': 'Subject Area'})\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average Weighted Proficiency by School Level and Content Area',\n",
    "    autosize=True,\n",
    "    width=1200,\n",
    "    height=900,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save the figure as HTML\n",
    "fig.write_html('../plotly_html/broad_overview.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
